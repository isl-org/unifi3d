# @package _global_

# to execute this experiment:
# accelerate launch scripts/accelerate/acc_train.py experiment=diffuse_doctree_vae_unet

defaults:
  - override /data: dualoctree_shape
  - override /model: diffusion
  - override /net_encode: doctree_vae
  - override /net_denoise: unet
  - override /diffusion_sampler: ddpm
  - override /trainer: default
  - override /logger:
    - aim

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["train", "dualoctree", "diffusion"]

seed: 12345
mixed_precision: "no"
octree_depth: 6
full_depth: 2
experiment_name: doctree_vae_unet_chair
categories: 
  - "chair"

data:
  train_batch_size: 420 # 4 gpus
  val_batch_size: 1
  test_batch_size: 1
  data_train:
    dataset:
      num_samples: -1
  data_test:
    dataset:
      num_samples: 1
  data_val:
    dataset:
      num_samples: 1

net_encode:
  ckpt_path: checkpoints/dualoctree_vae_chair.safetensors
  code_channel: 8 # Update this number based on the ckpt it uses.
  add_kl_loss: False

net_denoise:
  dims: 3
  image_size: 4 # 16
  in_channels: ${net_encode.code_channel} # 3
  out_channels: ${net_encode.code_channel} # 3
  model_channels: 192
  num_res_blocks: 2
  attention_resolutions:  [1, 2, 4] #[ 1,2,4 ] # 16, 8, 4
  # note: this isn\t actually the resolution but
  # the downsampling factor, i.e. this corresponds to
  # attention on spatial resolution 8,16,32, as the
  # spatial reolution of the latents is 64 for f4
  channel_mult: [1] # [ 1,2,4,4 ] adding 2 seems to have skip unevenly between w,h,d
  # num_head_channels: 32
  num_heads: 6

diffusion_sampler:
  beta_schedule: squaredcos_cap_v2
  clip_sample: False

model:
  encoder_decoder: ${net_encode}
  diff_model: ${net_denoise}
  sampling_scheduler: ${diffusion_sampler}
  # scheduler:
  #   _target_: torch.optim.lr_scheduler.LambdaLR
  #   lr_lambda:
  #     _target_: unifi3d.utils.scheduler.lambda_lr_utils.get_lambda_lr_poly
  #     max_epoch: ${trainer.max_epochs}
  #     lr_power: 0.9
  #   _partial_: true
  optimizer:
    lr: 0.0001
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 100
    gamma: 0.9

trainer:
  max_epochs: 3600
  min_epochs: 3600
  log_every_n_iter: 1
  check_val_every_n_epoch: -1
  save_checkpoint_every_n_epoch: 100
  ckpt_num: 3

paths:
  log_dir: ${paths.root_dir}/logs/${experiment_name}
  output_dir: ${paths.log_dir}
test: false

logger:
  aim:
    config:
      loggers: ["metric", "image", "hparams"] # metric, image, mesh, trimesh, video, hist, hparams
      tracked_variables:
        metric: ["loss", "lr"]
        image: ["render_gt", "render_rec"]